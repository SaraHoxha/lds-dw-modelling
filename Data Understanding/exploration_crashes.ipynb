{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv files\n",
    "crashes_df = pd.read_csv('../data/Crashes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape of crashes table\n",
    "crashes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check columns of crashes table\n",
    "crashes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of columns with missing values and their counts\n",
    "missing_values_dict = crashes_df.isna().sum()[crashes_df.isna().sum() > 0].to_dict()\n",
    "missing_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates in crashes table\n",
    "crashes_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value counts on missing values columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check all the values of each column using valueCount and check if there are columns that use a string to signal that the data is not known (e.g. weather condition just below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WEATHER_CONDITION\n",
       "CLEAR                     205435\n",
       "CLOUDY/OVERCAST             7573\n",
       "FOG/SMOKE/HAZE               549\n",
       "OTHER                        775\n",
       "RAIN                       23677\n",
       "SEVERE CROSS WIND GATE        49\n",
       "SLEET/HAIL                   342\n",
       "SNOW                        8276\n",
       "UNKNOWN                    11249\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check value counts for weather condition\n",
    "crashes_df.groupby('WEATHER_CONDITION').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check value counts for all columns\n",
    "for col in crashes_df.columns:\n",
    "    print (col)\n",
    "    print (crashes_df[col].value_counts())\n",
    "    print (\"_______________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns that use a replacement string (e.g Unknown) for null values are:\n",
    "- TRAFFIC_CONTROL_DEVICE\n",
    "- DEVICE_CONDITION\n",
    "- WEATHER_CONDITION\n",
    "- LIGHTING_CONDITION\n",
    "- TRAFFICWAY_TYPE\n",
    "- ROADWAY_SURFACE_COND\n",
    "- ROAD_DEFECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of columns that use a replacement string to signal missing values\n",
    "columns_with_strings_to_signal_missing_values = [\n",
    "    'TRAFFIC_CONTROL_DEVICE',\n",
    "    'DEVICE_CONDITION',\n",
    "    'WEATHER_CONDITION',\n",
    "    'LIGHTING_CONDITION',\n",
    "    'TRAFFICWAY_TYPE',\n",
    "    'ROADWAY_SURFACE_COND',\n",
    "    'ROAD_DEFECT'\n",
    "]\n",
    "\n",
    "# Iterating over the list of columns with replacement strings to print value counts\n",
    "for col in columns_with_strings_to_signal_missing_values:\n",
    "    print(f\"Value counts for {col}:\")\n",
    "    print(crashes_df[col].value_counts())\n",
    "    print(\"_______________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CRASH_HOUR\n",
    "- CRASH_DAY_OF_WEEK\n",
    "- CRASH_MONTH\n",
    "- POSTED_SPEED_LIMIT\n",
    "- TRAFFIC_CONTROL_DEVICE\n",
    "- WEATHER_CONDITION \n",
    "- LIGHTING_CONDITION\n",
    "- FIRST_CRASH_TYPE\n",
    "- TRAFFICWAY_TYPE\n",
    "- ALIGNMENT\n",
    "- ROADWAY_SURFACE_COND\n",
    "- ROAD_DEFECT\n",
    "- MOST_SEVERE_INJURY\n",
    "- PRIM_CONTRIBUTORY_CAUSE\n",
    "- SEC_CONTRIBUTORY_CAUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = [\n",
    "    'CRASH_HOUR',\n",
    "    'CRASH_DAY_OF_WEEK',\n",
    "    'CRASH_MONTH',\n",
    "    #'CRASH_DATE', #too many values and is already integrated in the other 3 columns above\n",
    "    'POSTED_SPEED_LIMIT',\n",
    "    'TRAFFIC_CONTROL_DEVICE',\n",
    "    'WEATHER_CONDITION',\n",
    "    'LIGHTING_CONDITION',\n",
    "    'FIRST_CRASH_TYPE',\n",
    "    'TRAFFICWAY_TYPE',\n",
    "    'ALIGNMENT',\n",
    "    'ROADWAY_SURFACE_COND',\n",
    "    'ROAD_DEFECT',\n",
    "    'MOST_SEVERE_INJURY',\n",
    "    'PRIM_CONTRIBUTORY_CAUSE',\n",
    "    'SEC_CONTRIBUTORY_CAUSE'\n",
    "]\n",
    "\n",
    "# Plotting each column\n",
    "for column in columns_to_plot:\n",
    "    value_counts = crashes_df[column].value_counts().sort_values()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    value_counts.plot(kind='bar')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the conclusion that come from the graph are:\n",
    "- the hours where most of the crashes happen is between 7 AM and 7 PM with a peak in the early afternoon (3PM to 5PM); one can see that 7AM and 7PM are the usual hours where people get in/out of work so it makes sense for more traffic.\n",
    "\n",
    "- the days of the week do not impact much on the number of crashes\n",
    "\n",
    "- there are more crashes during autumn/winter\n",
    "\n",
    "- there is a very noticeable peak of incidents based on the speed limit, with special regards to the 30 limit, still all really relevant data is contained between 0 and 45\n",
    "\n",
    "- other than when there are no controls the traffic signal and a very similar alternative such as the stop sign/flasher are the traffic control devices where most of the crashes happen\n",
    "\n",
    "- the weather conditions have almost a contrary impact on the number of incidents, where clear weather means more crashes. most likely people are more attentive to their driving when the weather is not optimal\n",
    "\n",
    "- the same thing that is valid for the weather condition is also valid for the lighting conditions, there are more crashes in the daylight than in the darkness/lighted road and all the other conditions combined\n",
    "\n",
    "- there is a peak of crashes when the road trafficway type is not divided\n",
    "\n",
    "- most of the accidents happen when the alignment of the road is straight and level (basically only that), but it is important to remember that the roads of Chicago don't change much in altitude (https://chatgpt.com/share/672beffe-678c-8003-9e93-e42a98c877dd (sources included)); maybe this column should be removed\n",
    "\n",
    "- as previously noted, when the conditions are best the drivers make the most mistakes, in fact when the roadway surface conditions are dry, rather than wet, snowy, ice, sand and other, the drivers have the most crushes for lack of attention to the road. Can the features that have this similar distribution be aggregated?\n",
    "\n",
    "- same as conditions of the surface also road defect get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features that can be aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- INJURIES_FATAL\n",
    "- INJURIES_INCAPACITATING\n",
    "- INJURIES_NON_INCAPACITATING\n",
    "- INJURIES_REPORTED_NOT_EVIDENT\n",
    "- INJURIES_NO_INDICATION\n",
    "- INJURIES_UNKNOWN\n",
    "after aggregating those INJURIES_TOTAL would be remvoed\n",
    "\n",
    "the values in all those features space in values in weird ways but i would propose to aggregate all the values in them into one single value for each type and put them into a scale to keep an order for them\n",
    "\n",
    "so maybe we would have\n",
    "- INJURIES_UNKNOWN 0\n",
    "- INJURIES_NO_INDICATION 1\n",
    "- INJURIES_REPORTED_NOT_EVIDENT 2\n",
    "- INJURIES_NON_INCAPACITATING 3\n",
    "- INJURIES_INCAPACITATING 4\n",
    "- INJURIES_FATAL 5\n",
    "\n",
    "-------------------------\n",
    "\n",
    "'CRASH_DATE' is an aggregation of: 'CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH'?\n",
    "\n",
    "-------------------------\n",
    "search for other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filling the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to find street direction since the similar named column in the vehicles dataset didnt yeld any good results (REMOVING FROM THE DATASET INSTEAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "might be able to get LATITUDE, LONGITUDE and LOCATION using the infos in STREET_NAME (and viceversa for the single street name value present) (REMOVING BECAUSE BEAT_OF_OCCURRENCE INTEGRATES THEM ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember to check if the street name have a null value in them (just check for the frequency of the street names, they should all be different(?), so if there is one repeating it probably is a mock value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crashes_df[\"STREET_NAME\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where STREET_NAME is 'WESTERN AVE'\n",
    "western_ave_street_no_counts = crashes_df[crashes_df['STREET_NAME'] == 'WESTERN AVE']['STREET_NO'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(western_ave_street_no_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "everything seems regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code to plot a correlation matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = crashes_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "# Set the title\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we find that:\n",
    "- most of the injuries are non incapacitating\n",
    "- there is a strong correlation between BEAT_OF_OCCURRENCE and LATITUDE and LONGITUDE\n",
    "- for the rest of the features there is not that much correlation, even tho, if the dataset was to be unified it would be interesting to check for the correlation matrix of all the dataset togheter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEAT_OF_OCCURRENCE is described as (in https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data):\n",
    "\t\n",
    "Chicago Police Department Beat ID. Boundaries available at https://data.cityofchicago.org/d/aerh-rz74 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as described by gpt:\n",
    "\n",
    "In Chicago, \"police beats\" are specific geographic areas within each police district. Each beat is assigned a dedicated police team responsible for routine patrols and responding to incidents. This organization allows officers to become familiar with the communities within their beat, enhancing local policing efforts and accountability. The City of Chicagoâ€™s data portal provides boundaries and GIS data for these police beats.\n",
    "\n",
    "does this mean that we can drop LONGITUDE, LATITUDE and LOCATION? (REMEMBER THAT THERE ARE 4 MISSING VALUES FOR BEAT_OF_OCCURRENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data type check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in crashes_df.columns:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Data Type: {crashes_df[col].dtype}\")\n",
    "    print(f\"Unique Values: {crashes_df[col].unique()}\")\n",
    "    print(\"_______________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to check if there are digits in the RD_NO column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df[\"RD_NO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the values seem to have a pattern, let's check if there is any value that doesn't match this pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df[~crashes_df['RD_NO'].str.match(r'^[A-Z]{2}\\d{6}$')][\"RD_NO\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the only 2 license plate that are not exactly matching the format are those 2 with lower case letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to check any weird value (does not match the formatting of the others) in LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df[\"LOCATION\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since all the values, at a first look, in the location column (except for the nan ones) are a string with this format\n",
    "- 'POINT (-87.716439109795 41.894718028422)'\n",
    "\n",
    "let's check if there are any different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for values that do not match the expected format\n",
    "not_nan_locations = crashes_df.dropna(subset=['LOCATION'])\n",
    "not_nan_locations[~not_nan_locations['LOCATION'].str.match(r\"^POINT \\(-?\\d+\\.\\d+ -?\\d+\\.\\d+\\)$\")][\"LOCATION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is no value that doesnt match the location that was required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "need to check if there are not date values in\n",
    "- CRASH_DATE,\n",
    "- DATE_POLICE_NOTIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values in CRASH_DATE match the pattern\n",
    "crash_date_pattern = r\"^\\d{2}/\\d{2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [AP]M$\"\n",
    "crash_date_check = crashes_df['CRASH_DATE'].str.match(crash_date_pattern)\n",
    "\n",
    "# Check if all values in DATE_POLICE_NOTIFIED match the pattern\n",
    "date_police_notified_pattern = r\"^\\d{2}/\\d{2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [AP]M$\"\n",
    "date_police_notified_check = crashes_df['DATE_POLICE_NOTIFIED'].str.match(date_police_notified_pattern)\n",
    "\n",
    "# Print results\n",
    "print(\"CRASH_DATE matches pattern:\", crash_date_check.all())\n",
    "print(\"DATE_POLICE_NOTIFIED matches pattern:\", date_police_notified_check.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(checked manually all the other types in the list in the output above and everything matches for sure (few values), other then the report one above which we're gonna check now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns that are float64 but might aswell be int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_columns = crashes_df.select_dtypes(include=['float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we convert the float to int when the values in the column are actually integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in float_columns:\n",
    "    # Check if the column contains any NaN or infinite values\n",
    "    if crashes_df[col].isnull().any() or np.isinf(crashes_df[col]).any():\n",
    "        print(f\"Column {col} contains NaN or infinite values, should skip conversion.\")\n",
    "        continue  # Skip this column if it has NaN or infinite values\n",
    "\n",
    "\n",
    "    # Check if all values are whole numbers\n",
    "    if (crashes_df[col] == crashes_df[col].astype('int')).all():\n",
    "        print(f\"Column {col} can be converted to int64.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weird values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "numeric_columns = crashes_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Iterate over each numeric column and sort values from max to min\n",
    "for col in numeric_columns:\n",
    "    print(f\"Sorted values for {col} (max to min):\")\n",
    "    sorted_values = crashes_df[col].sort_values(ascending=False)\n",
    "    print(sorted_values)\n",
    "    print(\"_______________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no weird values in numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = [item for item in list(crashes_df.columns) if item not in numeric_columns]\n",
    "non_numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in non_numeric_columns:\n",
    "    print(f\"Unique values in {col}:\\n\")\n",
    "    print(crashes_df[col].value_counts())\n",
    "    print(\"_______________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no weird values in the other columns, still need to check the date ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_columns = [\"CRASH_DATE\", \"DATE_POLICE_NOTIFIED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'CRASH_DATE' column to datetime\n",
    "crashes_df['DATE_POLICE_NOTIFIED_datetime'] = pd.to_datetime(crashes_df['DATE_POLICE_NOTIFIED'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Sort the dates from max to min\n",
    "df_sorted2 = crashes_df.sort_values(by='DATE_POLICE_NOTIFIED_datetime', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted2[\"CRASH_DATE_datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'CRASH_DATE' column to datetime\n",
    "crashes_df['CRASH_DATE_datetime'] = pd.to_datetime(crashes_df['CRASH_DATE'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Sort the dates from max to min\n",
    "df_sorted = crashes_df.sort_values(by='CRASH_DATE_datetime', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted[\"CRASH_DATE_datetime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No weird dates found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
